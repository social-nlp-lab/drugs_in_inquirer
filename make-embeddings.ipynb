{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make W2V embeddings over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "from scipy.spatial.distance import pdist\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2013,2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_articles = pickle.load(open(\"./data/drug_articles.pkl\", \"rb\"))\n",
    "# filter articles that are 2023\n",
    "drug_articles = drug_articles[drug_articles[\"Date\"].dt.year != 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for year in years:\n",
    "    # get the articles for the year\n",
    "    articles = drug_articles[drug_articles[\"Date\"].dt.year == year][\"clean_text\"]\n",
    "    # train the model\n",
    "    model = Word2Vec(\n",
    "        articles, window=15, min_count=1, \n",
    "        workers=4, vector_size=300, sg=0, \n",
    "        epochs=5, hs=0, sorted_vocab=1)\n",
    "    # save the model\n",
    "    # model.save(\"output/embeddings/word2vec_drug_\" + str(year) + \".model\")\n",
    "    # append the model to the list\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013 top words: \n",
      " ['police', 'new', 'drug', 'staff', 'years', 'philadelphia', 'inquirer', 'credit', 'email', 'county']\n",
      "2014 top words: \n",
      " ['police', 'new', 'drug', 'marijuana', 'county', 'staff', 'state', 'years', 'philadelphia', 'school']\n",
      "2015 top words: \n",
      " ['police', 'years', 'new', 'drug', 'philadelphia', 'staff', 'marijuana', 'city', 'credit', 'time']\n",
      "2016 top words: \n",
      " ['new', 'police', 'philadelphia', 'marijuana', 'drug', 'years', 'staff', 'state', 'medical', 'year']\n",
      "2017 top words: \n",
      " ['philadelphia', 'new', 'drug', 'state', 'city', 'marijuana', 'years', 'police', 'staff', 'like']\n",
      "2018 top words: \n",
      " ['marijuana', 'city', 'new', 'state', 'philadelphia', 'drug', 'year', 'like', 'years', 'staff']\n",
      "2019 top words: \n",
      " ['marijuana', 'new', 'drug', 'philadelphia', 'state', 'police', 'years', 'year', 'like', 'city']\n",
      "2020 top words: \n",
      " ['new', 'police', 'marijuana', 'philadelphia', 'drug', 'city', 'state', 'years', 'like', 'patients']\n",
      "2021 top words: \n",
      " ['philadelphia', 'new', 'state', 'drug', 'years', 'like', 'city', 'marijuana', 'police', 'year']\n",
      "2022 top words: \n",
      " ['marijuana', 'philadelphia', 'new', 'state', 'drug', 'pennsylvania', 'like', 'years', 'police', 'time']\n"
     ]
    }
   ],
   "source": [
    "# reduce the vocabulary to top 10000 words per year\n",
    "for i in range(len(years)):\n",
    "    models[i].wv.vectors = models[i].wv.vectors[:10000,:]\n",
    "    models[i].wv.index_to_key = models[i].wv.index_to_key[:10000]\n",
    "    models[i].wv.key_to_index = {k: models[i].wv.key_to_index[k] for k in models[i].wv.index_to_key[:10000]}\n",
    "    print(years[i], \"top words: \\n\", models[i].wv.index_to_key[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the models\n",
    "for i in range(len(years)):\n",
    "    models[i].save(\"output/embeddings/word2vec_drug_\" + str(years[i]) + \".model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## TEMPORAL EMBEDDING FROM PAPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2csc\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(drug_articles[\"clean_text\"])\n",
    "dictionary.filter_extremes(no_below=100)\n",
    "# 2,776 UNIQUE TOKENS\n",
    "\n",
    "corpus_full = [dictionary.doc2bow(text) for text in drug_articles[\"clean_text\"]]\n",
    "corpus_full = corpus2csc(corpus_full).transpose()\n",
    "corpus_full = csc_matrix(corpus_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_year = []\n",
    "for year in years:\n",
    "    # get the articles for the year\n",
    "    articles = drug_articles[drug_articles[\"Date\"].dt.year == year][\"clean_text\"]\n",
    "    # get the corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in articles]\n",
    "    corpora_year.append(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_year = []    \n",
    "for i in range(len(years)):\n",
    "    corpus = corpora_year[i]\n",
    "    cooc = np.zeros((len(dictionary), len(dictionary)))\n",
    "    for doc in corpus:\n",
    "        for word1 in doc:\n",
    "            for word2 in doc:\n",
    "                cooc[word1[0], word2[0]] += 1\n",
    "    cooc_year.append(cooc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2776, 2776)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooc_year[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cooc_year[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPMI stands for Positive Pointwise Mutual Information. It is a statistical measure used in natural language processing and information retrieval to capture the association between words in a text corpus. PPMI is a variant of the pointwise mutual information (PMI) measure, which quantifies the extent to which two words co-occur more or less frequently than would be expected by chance.\n",
    "\n",
    "The Positive Pointwise Mutual Information (PPMI) measure enhances the PMI by addressing the issue of negative PMI values. In standard PMI, if the co-occurrence of two words is less frequent than expected, the PMI value can become negative. PPMI resolves this by setting negative PMI values to zero, thus only considering positive associations between words.\n",
    "\n",
    "The PPMI measure is calculated based on the co-occurrence matrix of words in a corpus. Here's the formula for calculating PPMI:\n",
    "\n",
    "PPMI(w1, w2) = max(log2(P(w1, w2) / (P(w1) * P(w2))), 0)\n",
    "\n",
    "where:\n",
    "\n",
    "P(w1, w2) is the probability of observing the co-occurrence of words w1 and w2 in the corpus.\n",
    "P(w1) and P(w2) are the probabilities of observing words w1 and w2 individually in the corpus.\n",
    "The PPMI value reflects how strongly the co-occurrence of two words deviates from what would be expected by chance. *Higher PPMI values indicate a stronger association between words*, while zero values indicate no association.\n",
    "\n",
    "PPMI is often used in the context of building word embeddings or word representations. It can be used to construct a co-occurrence matrix and then apply dimensionality reduction techniques like singular value decomposition (SVD) to obtain dense, low-dimensional word vectors that capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppmi(coocur):\n",
    "\n",
    "  # Calculate the probability of each word appearing in the corpus.\n",
    "  p = np.sum(coocur, axis=1) / np.sum(coocur)\n",
    "\n",
    "  # Calculate the probability of each pair of words appearing in the corpus.\n",
    "  pq = np.sum(coocur, axis=0) / np.sum(coocur)\n",
    "\n",
    "  # Compute the PPMI value for each pair of words.\n",
    "  ppmi_matrix = np.log(pq / (p * p))\n",
    "\n",
    "  # Set all PPMI values less than 0 to 0.\n",
    "  ppmi_matrix[ppmi_matrix < 0] = 0\n",
    "\n",
    "  return ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the co-occurence matrix\n",
    "def get_ppmi(coocur):\n",
    "    co_occurence_matrix = coocur\n",
    "    # get the sum of the rows and columns\n",
    "    sum_rows = np.array(co_occurence_matrix.sum(axis=1)).flatten()\n",
    "    sum_columns = np.array(co_occurence_matrix.sum(axis=0)).flatten()\n",
    "    # get the total sum\n",
    "    total_sum = sum_rows.sum()\n",
    "    # get the PPMI matrix\n",
    "    ppmi_matrix = co_occurence_matrix.copy()\n",
    "    # get the indices of the non-zero elements\n",
    "    rows, cols = ppmi_matrix.nonzero()\n",
    "    # get the PPMI matrix\n",
    "    for row, col in zip(rows, cols):\n",
    "        # get the co-occurence\n",
    "        co_occurence = co_occurence_matrix[row, col]\n",
    "        # get the sum of the row and column\n",
    "        sum_row = sum_rows[row]\n",
    "        sum_col = sum_columns[col]\n",
    "        # compute the PMI\n",
    "        pmi = np.log(co_occurence) + np.log(total_sum) - np.log(sum_row) - np.log(sum_col)\n",
    "        # compute the PPMI\n",
    "        ppmi = max(0, pmi)\n",
    "        # set the value\n",
    "        ppmi_matrix[row, col] = ppmi\n",
    "    return ppmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_year = []\n",
    "for i in range(len(years)):\n",
    "    # get the co-occurence matrix\n",
    "    coocur = cooc_year[i]\n",
    "    # get the PPMI matrix\n",
    "    ppmi = get_ppmi(coocur)\n",
    "    # append to the list\n",
    "    ppmi_year.append(ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2776, 2776)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi_year[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import linalg\n",
    "def ppmi_svd(ppmi_matrix, k=300):\n",
    "  \"\"\"\n",
    "  Performs SVD on the ppmi matrix.\n",
    "\n",
    "  Args:\n",
    "    cooccurrence_matrix: A sparse matrix of PMI values.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of the singular vectors and singular values.\n",
    "  \"\"\"\n",
    "\n",
    "  # Calculate the ppmi for the cooccurrence matrix.\n",
    "  # ppmi_matrix = ppmi(cooccurrence_matrix)\n",
    "\n",
    "  # Subtract the minimum value from the ppmi matrix.\n",
    "  ppmi_matrix -= np.min(ppmi_matrix)\n",
    "\n",
    "  # Normalize the ppmi matrix.\n",
    "  ppmi_matrix /= np.sum(ppmi_matrix)\n",
    "\n",
    "  # Perform SVD on the normalized ppmi matrix.\n",
    "  u,s,vh = linalg.svds(ppmi_matrix, k=k)\n",
    "\n",
    "  return u,s,vh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first calculates the ppmi for the cooccurrence matrix. Then, it subtracts the minimum value from the ppmi matrix and normalizes it. Finally, it performs SVD on the normalized ppmi matrix and returns the singular vectors and singular values.\n",
    "\n",
    "The singular vectors represent the word embeddings. The first few singular vectors will capture the most important aspects of the PMI matrix. These vectors can be used to represent words in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_year = []\n",
    "for i in range(len(years)):\n",
    "    u,s,vh = ppmi_svd(ppmi_year[i])\n",
    "    embed_year.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "for i in range(len(years)):\n",
    "    np.save(\"output/embeddings/svd_drug_\" + str(years[i]) + \".npy\", embed_year[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
