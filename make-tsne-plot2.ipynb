{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Dynamic Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint\n",
    "from scipy.spatial.distance import pdist\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2013,2023)\n",
    "dir = \"output/models/word2vec_drug_\"\n",
    "\n",
    "models = []\n",
    "for year in years:\n",
    "    models.append(Word2Vec.load(dir+str(year)+\".model\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From \"Diachronic Word Embeddings Reveal Statistical Lawso f Semantic Change\" by Jurafsky et al. to explore semantic changes of target words over time\n",
    "\n",
    "`Low-dimensional embeddings will not be naturally aligned due to the non-unique nature of the SVD and the stochastic nature of SGNS. In particular, both these methods may result in arbitrary orthogonal transformations, which do not affect pairwise cosine-similarities within-years but will preclude comparison of the same word across time.`\n",
    "\n",
    "We use orthogonal Procrustes to align the learned low-dimensional embeddings. Defining W(t) ∈ Rd×|V| as the matrix of word embeddings learned at year t, we align across time-periods while preserving cosine similarities by optimizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_articles = pickle.load(open(\"./data/drug_articles.pkl\", \"rb\"))\n",
    "# filter articles that are 2023\n",
    "drug_articles = drug_articles[drug_articles[\"Date\"].dt.year != 2023]\n",
    "# get the years\n",
    "years = drug_articles[\"Date\"].dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for year in years:\n",
    "    # get the articles for the year\n",
    "    articles = drug_articles[drug_articles[\"Date\"].dt.year == year][\"clean_text\"]\n",
    "    # train the model\n",
    "    model = Word2Vec(\n",
    "        articles, window=15, min_count=1, \n",
    "        workers=4, vector_size=300, sg=0, \n",
    "        epochs=5, hs=0, sorted_vocab=1)\n",
    "    # save the model\n",
    "    model.save(\"output/models/word2vec_drug_\" + str(year) + \".model\")\n",
    "    # append the model to the list\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = [\"said\", \"drug\"]\n",
    "\n",
    "# remove stopwords before training\n",
    "for model in models:\n",
    "    for stopword in extra_stopwords:\n",
    "        model.wv.key_to_index.pop(stopword, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013 top words: \n",
      " ['said', '-', 'said.', 'police', 'new', '.', 'drug', '\"i', 'staff', 'inquirer']\n",
      "2014 top words: \n",
      " ['said', '-', 'said.', 'new', 'drug', 'police', '\"i', '.', 'state', 'staff']\n",
      "2015 top words: \n",
      " ['said', '-', 'said.', 'new', 'police', 'drug', '\"i', 'staff', 'marijuana', 'philadelphia']\n",
      "2016 top words: \n",
      " ['said', '-', 'said.', 'new', 'police', 'drug', 'marijuana', 'people', 'staff', '\"i']\n",
      "2017 top words: \n",
      " ['said', '-', 'said.', 'people', 'new', 'drug', 'staff', 'state', 'philadelphia', 'marijuana']\n",
      "2018 top words: \n",
      " ['said', '-', 'said.', 'people', 'marijuana', 'new', 'drug', 'city', 'state', 'medical']\n",
      "2019 top words: \n",
      " ['said', '-', 'said.', 'people', 'new', 'marijuana', 'drug', 'philadelphia', 'state', 'like']\n",
      "2020 top words: \n",
      " ['said', '-', 'people', 'said.', 'new', 'police', 'marijuana', 'drug', 'state', 'like']\n",
      "2021 top words: \n",
      " ['said', '—', 'said.', 'people', 'new', 'drug', 'like', 'state', 'marijuana', 'medical']\n",
      "2022 top words: \n",
      " ['said', '—', 'said.', 'people', 'new', 'marijuana', 'drug', 'state', 'like', 'medical']\n"
     ]
    }
   ],
   "source": [
    "# reduce the vocabulary to top 10000 words per year\n",
    "for i in range(len(years)):\n",
    "    models[i].wv.vectors = models[i].wv.vectors[:10000,:]\n",
    "    models[i].wv.index_to_key = models[i].wv.index_to_key[:10000]\n",
    "    models[i].wv.key_to_index = {k: models[i].wv.key_to_index[k] for k in models[i].wv.index_to_key[:10000]}\n",
    "    print(years[i], \"top words: \\n\", models[i].wv.index_to_key[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 300) (10000, 300) (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "# align the embeddings using Procrustes\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "# def align_embeddings(embeddings):\n",
    "#     # Get the embedding matrices\n",
    "#     matrices = [embedding.wv.vectors for embedding in embeddings]\n",
    "\n",
    "#     # Align the embeddings\n",
    "#     aligned = orthogonal_procrustes(matrices[0], matrices[1])\n",
    "#     aligned_matrices = [aligned[1]]  # Initialize with the first aligned matrix\n",
    "\n",
    "#     # Iterate over the remaining embeddings and align them with the previous aligned matrix\n",
    "#     for matrix in matrices[2:]:\n",
    "#         aligned = orthogonal_procrustes(aligned_matrices[-1], matrix)\n",
    "#         aligned_matrices.append(aligned[1])\n",
    "\n",
    "#     # Return the aligned embedding matrices\n",
    "#     return aligned_matrices\n",
    "\n",
    "# # models have different dimensions\n",
    "# align them\n",
    "print(models[0].wv.vectors.shape,\n",
    "models[1].wv.vectors.shape, models[2].wv.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = [embedding.wv.vectors for embedding in models]\n",
    "\n",
    "# align all 10 embeddings\n",
    "\n",
    "for i in range(1, len(matrices)):\n",
    "    aligned = orthogonal_procrustes(matrices[0], matrices[i])\n",
    "    matrices[i] = aligned[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem: aligning the matrices using orthogonal Procrustes requires the matrices to be of the same size. However, the number of words in the vocabulary changes over time. We solve this problem by aligning the matrices of the top 10,000 words in each time period. We could also remove words that are not present in at least 3 time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected ndim to be 2, but observed 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drugs_in_inquirer/make-tsne-plot2.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drugs_in_inquirer/make-tsne-plot2.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m aligned \u001b[39m=\u001b[39m align_embeddings(models)\n",
      "\u001b[1;32m/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drugs_in_inquirer/make-tsne-plot2.ipynb Cell 11\u001b[0m in \u001b[0;36malign_embeddings\u001b[0;34m(embeddings)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drugs_in_inquirer/make-tsne-plot2.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Iterate over the remaining embeddings and align them with the previous aligned matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drugs_in_inquirer/make-tsne-plot2.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m matrix \u001b[39min\u001b[39;00m matrices[\u001b[39m2\u001b[39m:]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drugs_in_inquirer/make-tsne-plot2.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     aligned \u001b[39m=\u001b[39m orthogonal_procrustes(aligned_matrices[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], matrix)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drugs_in_inquirer/make-tsne-plot2.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     aligned_matrices\u001b[39m.\u001b[39mappend(aligned[\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laylabouzoubaa/Projects/PHD_RESEARCH/drugs_in_inquirer/make-tsne-plot2.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Return the aligned embedding matrices\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/linalg/_procrustes.py:81\u001b[0m, in \u001b[0;36morthogonal_procrustes\u001b[0;34m(A, B, check_finite)\u001b[0m\n\u001b[1;32m     79\u001b[0m     B \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(B)\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m A\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mexpected ndim to be 2, but observed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m A\u001b[39m.\u001b[39mndim)\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m A\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m B\u001b[39m.\u001b[39mshape:\n\u001b[1;32m     83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mthe shapes of A and B differ (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m     84\u001b[0m         A\u001b[39m.\u001b[39mshape, B\u001b[39m.\u001b[39mshape))\n",
      "\u001b[0;31mValueError\u001b[0m: expected ndim to be 2, but observed 0"
     ]
    }
   ],
   "source": [
    "aligned = align_embeddings(models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. compute cosine distance between word vectors for each year \n",
    "2. compute average cosine distance between word vectors for each year\n",
    "3. compute average cosine distance between word vectors for each year and the previous year\n",
    "4. compute average cosine distance between word vectors for each year and the first year"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other stuff\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use UMAP to reduce the dimensionality\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15, metric='cosine', random_state=42)\n",
    "# fit the reducer\n",
    "red_embedding = reducer.fit_transform(models[0].wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40458, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laylabouzoubaa/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:795: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/laylabouzoubaa/opt/anaconda3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:805: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_embedding = tsne.fit_transform(models[0].wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40458, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def reduce_embeddings(embeddings):\n",
    "    # Determine the maximum dimension among the word embeddings\n",
    "    max_dim = max(embedding.vector_size for embedding in embeddings)\n",
    "    \n",
    "    # Concatenate the embeddings into a single matrix with aligned dimensions\n",
    "    aligned_embeddings = []\n",
    "    for embedding in embeddings:\n",
    "        # Pad or truncate the embedding to match the maximum dimension\n",
    "        padded_embedding = np.pad(embedding.wv.vectors, ((0, 0), (0, max_dim - embedding.vector_size)), mode='constant')\n",
    "        aligned_embeddings.append(padded_embedding)\n",
    "    \n",
    "    combined_embeddings = np.concatenate(aligned_embeddings, axis=1)\n",
    "    \n",
    "    # Apply PCA with n_components set to the desired dimension (here, 10,000)\n",
    "    pca = PCA(n_components=10000)\n",
    "    reduced_embeddings = pca.fit_transform(combined_embeddings)\n",
    "    \n",
    "    return reduced_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10000)\n",
    "reduced_embeddings = pca.fit_transform(models[0].wv.vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the reduce_embeddings function first determines the maximum dimension among the word embeddings in the input list. Then, each word embedding is either padded or truncated to match the maximum dimension using np.pad. This ensures that all embeddings have the same shape along the feature axis.\n",
    "\n",
    "The padded or truncated embeddings are stored in the aligned_embeddings list, which is then concatenated along the feature axis to create the combined_embeddings matrix. The rest of the code remains the same as before, applying PCA to reduce the dimensionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9368c0a12bf520716c0c4f259a27602b49dc562e49a779d524d90b4c1cdf86a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
